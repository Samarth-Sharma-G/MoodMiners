{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading \n",
    "Using a single zip \"Actors 01-05\" as a test for loading and creating the full metadata dataset on the larger corpus of audio .wav files, for simplicity and to avoid using excessive memory.\n",
    "\n",
    "Directories have the following structure:\n",
    "\n",
    "- Actors_1-5 (top level, many of these with name structure Actors_6-10, Actors_11_15, etc.)\n",
    "    - Actor_01 (second level, many of these with name structure Actor_01, Actor_02, etc.)\n",
    "       - 03-01-01-01-01-01-01.wav (third level, each file is a wav audio file, and the names contain the feature to be extracted)\n",
    "          > Many of these audio files follow the same naming structure but are not sequential.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import json\n",
    "# Mood Miners Emotion Detection Project\n",
    "# Description: This file contains the code to load the data from the zip file\n",
    "\n",
    "def process_audio_from_zip(zip_path):\n",
    "    \"\"\"\n",
    "    This function takes in the path to a zip file containing audio files.\n",
    "    It then processes the audio data using librosa and creates and returns a dataframe.\n",
    "    \"\"\"\n",
    "    # set extract path to name of zip\n",
    "    extract_path = zip_path.split('.')[0]\n",
    "    # create the directory if it doesn't exist\n",
    "    if not os.path.exists(extract_path):\n",
    "        print('Extracting zip file ' + zip_path.split('/')[1] + ' to ' + zip_path.split('.')[0])\n",
    "        os.makedirs(extract_path)\n",
    "\n",
    "        # extract the contents of the zip file to the directory\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "    # first, simple audio features used by librosa, excluding the raw data\n",
    "    # features: tempo, spectral_centroid, spectral_bandwidth, spectral_rolloff, zero_crossing_rate\n",
    "    #           chroma_stft, mfcc, rmse\n",
    "\n",
    "    # second, inherent emotion features\n",
    "    # creating the feature dictionary to return later as a df\n",
    "    feature_dict = {'actor': [], 'tempo': [], 'y':[], 'sr':[], 'onset_env':[], 'spectral_centroid': [], 'spectral_bandwidth':[], 'spectral_rolloff':[], 'zero_crossing_rate':[], 'chroma_stft':[],\n",
    "                    'mfcc':[], 'rmse':[], 'modality':[], 'vocal_channel':[], 'emotion':[], 'emotional_intensity':[], 'statement':[],'repetition':[]\n",
    "    }\n",
    "    # for each file in the directory ill be inserting the data into the feature dictionary\n",
    "    for actor_dir in os.listdir(extract_path):\n",
    "        if not actor_dir.startswith('Actor'):\n",
    "            continue\n",
    "        print('Processing the actor directory: ' + actor_dir)\n",
    "        for wav_file in os.listdir(extract_path + '/' + actor_dir):\n",
    "            if not wav_file.endswith('.wav'):\n",
    "                continue\n",
    "            ### Process Librosa Features ###\n",
    "            # load the audio file\n",
    "            y, sr = librosa.load(extract_path + '/' + actor_dir + '/' + wav_file)\n",
    "            # calculate the tempo\n",
    "            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "            # calculate the spectral centroid\n",
    "            spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "            # calculate the spectral bandwidth\n",
    "            spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "            # calculate the spectral rolloff\n",
    "            spec_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "            # calculate the zero crossing rate\n",
    "            zcr = librosa.feature.zero_crossing_rate(y)\n",
    "            # calculate the chroma stft\n",
    "            chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "            # calculate the mfcc\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "            # calculate the rmse\n",
    "            rmse = librosa.feature.rms(y=y)\n",
    "            # calculate onset strength\n",
    "            onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "            # add all the features to the dictionary\n",
    "            # appending the features to the dictionary list with the following keys [tempo, spec_cent, spec_bw, spec_rolloff, zcr, chroma_stft, mfcc, rmse]\n",
    "            feature_dict['actor'].append(actor_dir)\n",
    "            feature_dict['tempo'].append(tempo)\n",
    "            feature_dict['spectral_centroid'].append(spec_cent)\n",
    "            feature_dict['spectral_bandwidth'].append(spec_bw)\n",
    "            feature_dict['spectral_rolloff'].append(spec_rolloff)\n",
    "            feature_dict['zero_crossing_rate'].append(zcr)\n",
    "            feature_dict['chroma_stft'].append(chroma_stft)\n",
    "            feature_dict['mfcc'].append(mfcc)\n",
    "            feature_dict['rmse'].append(rmse)\n",
    "            feature_dict['onset_env'].append(onset_env)\n",
    "            feature_dict['y'].append(y)\n",
    "            feature_dict['sr'].append(sr)\n",
    "\n",
    "            ### Process Inherent Emotion Features ###\n",
    "            identifiers_only = wav_file.split('.')[0].split('-')\n",
    "            # Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "            feature_dict['modality'].append(identifiers_only[0])\n",
    "            # Vocal channel (01 = speech, 02 = song).\n",
    "            feature_dict['vocal_channel'].append(identifiers_only[1])\n",
    "            # Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "            feature_dict['emotion'].append(identifiers_only[2])\n",
    "            # Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "            feature_dict['emotional_intensity'].append(identifiers_only[3])\n",
    "            # Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "            feature_dict['statement'].append(identifiers_only[4])\n",
    "            # Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "            feature_dict['repetition'].append(identifiers_only[5])\n",
    "\n",
    "        print(\"Finished processing the actor directory: \" + actor_dir)\n",
    "    print(\"Finished processing all the audio files in the zip file \" + zip_path.split('/')[1])\n",
    "    \n",
    "    # deleting the root directory after processing\n",
    "    print(\"Deleting the root directory \" + extract_path)\n",
    "    shutil.rmtree(extract_path)\n",
    "    \n",
    "    actor_audio_df = pd.DataFrame(feature_dict)\n",
    "    return actor_audio_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting zip file Actors_1-5.zip to emotiona_speech/Actors_1-5\n",
      "Processing the actor directory: Actor_04\n",
      "Finished processing the actor directory: Actor_04\n",
      "Processing the actor directory: Actor_03\n",
      "Finished processing the actor directory: Actor_03\n",
      "Processing the actor directory: Actor_02\n",
      "Finished processing the actor directory: Actor_02\n",
      "Processing the actor directory: Actor_05\n",
      "Finished processing the actor directory: Actor_05\n",
      "Processing the actor directory: Actor_01\n",
      "Finished processing the actor directory: Actor_01\n",
      "Finished processing all the audio files in the zip file Actors_1-5.zip\n",
      "Deleting the root directory emotiona_speech/Actors_1-5\n"
     ]
    }
   ],
   "source": [
    "audio_metadata_shard = process_audio_from_zip('emotiona_speech/Actors_1-5.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the list of list features to json and then string so that it can be stored in a csv as a string with full fidelity\n",
    "audio_metadata_shard['spectral_centroid'] = audio_metadata_shard['spectral_centroid'].apply(lambda x: json.dumps(x.tolist()))\n",
    "audio_metadata_shard['spectral_bandwidth'] = audio_metadata_shard['spectral_bandwidth'].apply(lambda x: json.dumps(x.tolist()))\n",
    "audio_metadata_shard['spectral_rolloff'] = audio_metadata_shard['spectral_rolloff'].apply(lambda x: json.dumps(x.tolist()))\n",
    "audio_metadata_shard['zero_crossing_rate'] = audio_metadata_shard['zero_crossing_rate'].apply(lambda x: json.dumps(x.tolist()))\n",
    "audio_metadata_shard['chroma_stft'] = audio_metadata_shard['chroma_stft'].apply(lambda x: json.dumps(x.tolist()))\n",
    "audio_metadata_shard['mfcc'] = audio_metadata_shard['mfcc'].apply(lambda x: json.dumps(x.tolist()))\n",
    "audio_metadata_shard['rmse'] = audio_metadata_shard['rmse'].apply(lambda x: json.dumps(x.tolist()))\n",
    "audio_metadata_shard['onset_env'] = audio_metadata_shard['onset_env'].apply(lambda x: json.dumps(x.tolist()))\n",
    "audio_metadata_shard['y'] = audio_metadata_shard['y'].apply(lambda x: json.dumps(x.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas option to prevent truncation when writing to csv for processing the list of lists in the dataframe with ast.literal_eval\n",
    "#pd.set_option('display.max_seq_items', None)\n",
    "audio_metadata_shard.to_csv('Actors_1-5_Metadata.csv', index=False)\n",
    "#pd.reset_option('display.max_seq_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_metadata_shard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After processing of the audio data into a metadata CSV dataset for the Actors 1-5, the size of the raw audio dataset/directory reduced from 125 MB to only 5 MB, which is a reduction of about 0.96%\n"
     ]
    }
   ],
   "source": [
    "print(\"After processing of the audio data into a metadata CSV dataset for the Actors 1-5, the size of the raw audio dataset/directory reduced from 125 MB to only 5 MB, which is a reduction of about {}%\".format(1 - 5/125))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing that the shards are converted to the correct format (json string)\n",
    "audio_metadata_shard = pd.read_csv('Actors_1-5_Metadata.csv')\n",
    "# convert the json stringa back to a list of lists for the list of list features using json.loads\n",
    "audio_metadata_shard['spectral_centroid'] = audio_metadata_shard['spectral_centroid'].apply(lambda x: json.loads(x))\n",
    "audio_metadata_shard['spectral_bandwidth'] = audio_metadata_shard['spectral_bandwidth'].apply(lambda x: json.loads(x))\n",
    "audio_metadata_shard['spectral_rolloff'] = audio_metadata_shard['spectral_rolloff'].apply(lambda x: json.loads(x))\n",
    "audio_metadata_shard['zero_crossing_rate'] = audio_metadata_shard['zero_crossing_rate'].apply(lambda x: json.loads(x))\n",
    "audio_metadata_shard['chroma_stft'] = audio_metadata_shard['chroma_stft'].apply(lambda x: json.loads(x))\n",
    "audio_metadata_shard['mfcc'] = audio_metadata_shard['mfcc'].apply(lambda x: json.loads(x))\n",
    "audio_metadata_shard['rmse'] = audio_metadata_shard['rmse'].apply(lambda x: json.loads(x))\n",
    "audio_metadata_shard['onset_env'] = audio_metadata_shard['onset_env'].apply(lambda x: json.loads(x))\n",
    "audio_metadata_shard['y'] = audio_metadata_shard['y'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.800436600926332e-06,\n",
       " 7.78237335907761e-06,\n",
       " -2.2437436655309284e-06,\n",
       " 9.221363939104776e-07,\n",
       " -2.9372728249654756e-07]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_metadata_shard['y'][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Given that the function was able to process the data for the Actors 1-5 zip successfully, we can now go on to write a complete script to process the entire raw data directory corpus (for Actors 1-24, excluding the additional emotiona_speech/audio_speech_actors_01-24 directory - this one may contain different features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this after running python sample.py\n",
    "meta_df = pd.read_csv('actors_meta_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
